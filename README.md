# ğŸ§  Cerebro: Autonomous Product and Marketing DS

**Cerebro** is an autonomous, multi-agent system that generates production-grade data science code for product and marketing analyticsâ€”no templates, no hardcoding, just intelligent code generation.

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![JAX](https://img.shields.io/badge/JAX-Accelerated-orange.svg)](https://github.com/google/jax)
[![NumPyro](https://img.shields.io/badge/NumPyro-Bayesian-green.svg)](https://num.pyro.ai/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Cerebro** is a fully agentic Marketing Mix Modeling (MMM) system that autonomously generates, validates, and executes high-quality Bayesian models. Built with AI agents that reason, self-heal, and adapt.

## ğŸ¯ What Makes Cerebro Different

- **Fully Agentic**: No hardcoded templates - AI generates all code from data
- **Self-Validating**: Multi-layer validation with automatic error fixing (15 retries)
- **Self-Healing**: Execution validator fixes runtime errors using RAG + few-shot learning
- **Robust**: MCMC/NUTS for Bayesian inference (like Google Meridian)
- **Backend-Aware**: Adapts to NumPyro MCMC, SVI, or JAX optimization
- **Zero Hardcoding**: All configuration from spec, all code from AI

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/yourusername/cerebro.git
cd cerebro

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup Credentials

Choose one of three methods:

**Option A: Config File (Recommended)**
```bash
cp .api_config.yaml.example .api_config.yaml
# Edit .api_config.yaml with your API key
```

**Option B: Environment Variable**
```bash
export OPENAI_API_KEY="sk-your-key-here"
```

**Option C: Command Line**
```bash
python3 run_pipeline.py --api-key "sk-your-key-here"
```

### 3. Run the Pipeline

```bash
# Basic usage (uses config file)
python3 run_pipeline.py

# With custom data
python3 run_pipeline.py --data-path "your_data.csv"

# With explicit credentials
python3 run_pipeline.py \
  --api-key "sk-..." \
  --base-url "https://api.openai.com/v1" \
  --data-path "your_data.csv"
```

### 4. What Happens Next

The system will:
1. **Analyze your data** - Detect channels, controls, date columns
2. **Generate spec** - Create MMMSpec YAML configuration
3. **Generate code** - Create 6 modules with AI (exploration, preprocessing, modeling, etc.)
4. **Validate code** - Multi-layer validation with self-healing
5. **Execute models** - Run Bayesian MCMC inference
6. **Create visualizations** - Generate plots and diagnostics

**Total Time**: ~30-45 minutes (mostly MCMC sampling)

## ğŸ“Š Features

### Agentic System
- **SpecWriterAgent**: Analyzes data, generates MMM specifications
- **ModelingAgent**: Creates Bayesian models with transformations
- **ValidationAgents**: Multi-layer validation (static â†’ API â†’ JAX â†’ execution)
- **Self-Healing**: Automatically fixes errors using RAG + LLM reasoning

### Bayesian Modeling
- **MCMC/NUTS**: Full posterior uncertainty quantification
- **NumPyro SVI**: Scalable variational inference
- **JAX Optimization**: Fast MAP estimation
- **Transformations**: Geometric adstock, Hill saturation
- **Data-Driven Priors**: Automatic prior scaling

### Run & Config
- **Multi-Tier Credentials**: CLI args â†’ env vars â†’ config file
- **Backend Detection**: Adapts to numpyro_nuts, numpyro_svi, jax_optim
- **Parallel MCMC**: Multi-chain sampling for faster convergence
- **RAG Integration**: 4,224+ MMM examples
- **Comprehensive Docs**: All context in CEREBRO_COMPLETE_DOCUMENTATION.md

## ğŸ—ï¸ Architecture

```
User Data (CSV)
    â†“
SpecWriterAgent â†’ Analyzes data â†’ Generates MMMSpec
    â†“
Pipeline Agents (6 modules)
    â†“
â”œâ”€ DataExplorationAgent â†’ EDA code
â”œâ”€ PreprocessingAgent â†’ Data prep code
â”œâ”€ ModelingAgent â†’ Bayesian model code
â”‚   â”œâ”€ HybridValidator
â”‚   â”‚   â”œâ”€ Static validation
â”‚   â”‚   â”œâ”€ API validation
â”‚   â”‚   â”œâ”€ JAX tracing
â”‚   â”‚   â””â”€ Execution validation (self-healing)
â”‚   â””â”€ Executes MCMC/NUTS
â”œâ”€ DiagnosticsAgent â†’ Model diagnostics
â”œâ”€ OptimizationAgent â†’ Budget optimization
â””â”€ VisualizationAgent â†’ Plots & charts
    â†“
Results (PKL, plots, reports)
```

## ğŸ“– Documentation

- **[CEREBRO_COMPLETE_DOCUMENTATION.md](CEREBRO_COMPLETE_DOCUMENTATION.md)** - Complete system documentation (24 KB, 806 lines)
- **[SECURITY.md](SECURITY.md)** - Credential management guide
- **[USAGE.md](USAGE.md)** - Quick reference guide

## ğŸ”§ Configuration

### Data Requirements

Your CSV should have:
- **Date column** (any frequency: daily, weekly, monthly)
- **Target column** (outcome variable: sales, visits, revenue)
- **Channel columns** (media spend or impressions with keywords: "spend", "impression", "cost")
- **Control variables** (optional: price, promotions, holidays)

**Example:**
```csv
Date,target_visits,impressions_Channel_01,impressions_Channel_02,price,promotion
2024-01-01,1000,50000,30000,10.99,0
2024-01-02,1200,55000,32000,10.99,1
...
```

### Spec Configuration

The system auto-generates `spec_auto.yaml`:

```yaml
outcome: target_visits
date_column: Date
channels:
  - name: impressions_Channel_01
    transform:
      adstock:
        type: geometric
        max_lag: 6
      saturation:
        type: hill
controls:
  - price
  - promotion
inference:
  backend: numpyro_nuts  # or numpyro_svi, jax_optim
  num_warmup: 500
  num_samples: 500
  num_chains: 1
```

## ğŸ“ Example Queries

### Check Model Results
```python
import pickle
with open('module_3_results.pkl', 'rb') as f:
    results = pickle.load(f)

print(f"RÂ²: {results['r2']:.4f}")
print(f"Channels: {results['channel_names']}")
print(f"Coefficients: {results['params']}")
```

### Customize MCMC
Edit `spec_auto.yaml`:
```yaml
inference:
  backend: numpyro_nuts
  num_warmup: 1000      # More warmup for complex models
  num_samples: 1000     # More samples for better posteriors
  num_chains: 4         # Parallel chains (requires numpyro.set_host_device_count(4))
```

### Use Different Backend
```yaml
inference:
  backend: numpyro_svi  # Faster approximate inference
  num_steps: 10000
  learning_rate: 0.01
```

## ğŸ”¬ Advanced Usage

### Custom Credential Method

```python
from cerebro.llm.api_backend import ApiBackend

# Direct instantiation
llm = ApiBackend(
    api_key="sk-...",
    base_url="https://api.openai.com/v1",
    model="gpt-4o"
)
```

### Run Specific Modules

```python
from cerebro.spec.schema import MMMSpec
from cerebro.llm.api_backend import ApiBackend
from cerebro.agents.modeling_agent import ModelingAgent

# Load spec
spec = MMMSpec.from_yaml('spec_auto.yaml')

# Initialize agent
llm = ApiBackend()
modeling_agent = ModelingAgent(llm, use_rag=True)

# Generate model code
model_code = modeling_agent.generate_model_code(
    spec=spec,
    data_path='preprocessed_data.csv',
    enable_validation=True
)

# Execute
exec_globals = {}
exec(model_code, exec_globals)
results = exec_globals['run_modeling']('preprocessed_data.csv')
```

### Add Custom Transformations

The system auto-generates transformations based on spec, but you can customize:

```yaml
channels:
  - name: TV
    transform:
      adstock:
        type: geometric  # or: delayed, carryover, exponential
        max_lag: 8       # Custom lag
      saturation:
        type: hill       # or: logistic, exponential
```

## ğŸ“ˆ Performance

**Validation Speed:**
- Static validation: <1 second
- Execution validation: ~30 seconds (minimal MCMC: 10 warmup + 10 samples)
- Full MCMC: ~20-30 minutes (500 warmup + 500 samples Ã— 1 chain)

**Model Quality:**
- Typical RÂ²: 0.95-0.99 on real MMM data
- Acceptance rate: 0.85-0.95 (well-conditioned)
- Divergences: <1% (healthy MCMC)

**System Stats:**
- Code generation: ~15 seconds per module
- Total pipeline: ~30-45 minutes end-to-end
- Self-healing success: >90% within 15 retries

## ğŸ§ª Testing

```bash
# Test core imports
python3 -c "
from cerebro.agents.spec_writer_agent import AutonomousSpecWriter
from cerebro.agents.modeling_agent import ModelingAgent
from cerebro.llm.api_backend import ApiBackend
print('All imports successful!')
"

# Run full pipeline (test mode)
python3 run_pipeline.py --data-path "examples/MMM Data.csv"
```

## ğŸ› ï¸ Troubleshooting

### Slow MCMC (1023 leapfrog steps)

**Symptom**: Each sample takes 15-20 seconds

**Cause**: Poorly conditioned posterior

**Fix**: The system automatically scales priors, but if issues persist:
```python
# In generated model code, priors are already scaled by sqrt(n_channels)
# If still slow, try:
# 1. Use LogNormal(0, 0.5) for transformation parameters
# 2. Scale data to [0, 1]
# 3. Reduce num_chains to 1
```

### NaN Loss

**Symptom**: RuntimeError: NaN detected in loss

**Cause**: Exploding gradients or poor priors

**Fix**: System auto-fixes this, but manual fix:
```python
# System automatically:
# 1. Scales inputs to [0, 1]
# 2. Uses data-driven priors
# 3. Applies gradient clipping
```

### API Credentials Not Found

**Symptom**: ValueError: API key not found

**Fix**:
```bash
# Option 1: Config file
cp .api_config.yaml.example .api_config.yaml
# Edit with your key

# Option 2: Environment variable
export OPENAI_API_KEY="sk-..."

# Option 3: Command line
python3 run_pipeline.py --api-key "sk-..."
```

## ğŸ“¦ Project Structure

```
cerebro/
â”œâ”€â”€ README.md                              â† You are here
â”œâ”€â”€ CEREBRO_COMPLETE_DOCUMENTATION.md     â† Full system docs
â”œâ”€â”€ SECURITY.md                            â† Credential management
â”œâ”€â”€ USAGE.md                               â† Quick reference
â”œâ”€â”€ .api_config.yaml.example              â† Config template
â”œâ”€â”€ run_pipeline.py                   â† Main pipeline
â”œâ”€â”€ cerebro/
â”‚   â”œâ”€â”€ agents/                            â† AI agents
â”‚   â”‚   â”œâ”€â”€ spec_writer_agent.py          â† Spec generation
â”‚   â”‚   â”œâ”€â”€ modeling_agent.py             â† Model generation
â”‚   â”‚   â”œâ”€â”€ data_exploration_agent.py     â† EDA
â”‚   â”‚   â”œâ”€â”€ preprocessing_agent.py        â† Data prep
â”‚   â”‚   â”œâ”€â”€ diagnostics_agent.py          â† Model diagnostics
â”‚   â”‚   â”œâ”€â”€ optimization_agent.py         â† Budget optimization
â”‚   â”‚   â””â”€â”€ visualization_agent.py        â† Plotting
â”‚   â”œâ”€â”€ llm/                               â† LLM backends
â”‚   â”‚   â””â”€â”€ api_backend.py                â† OpenAI/compatible APIs
â”‚   â”œâ”€â”€ spec/                              â† Schema definitions
â”‚   â”‚   â””â”€â”€ schema.py                     â† MMMSpec Pydantic model
â”‚   â””â”€â”€ utils/                             â† Utilities
â””â”€â”€ examples/                              â† Example scripts
```

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“œ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- **NumPyro**: Bayesian inference framework
- **JAX**: Accelerated computing
- **Google Meridian**: Inspiration for MCMC architecture
- **PyMC-Marketing**: MMM best practices
- **OpenAI**: GPT-4o for agentic reasoning

## ğŸ“ Support

- **Documentation**: See [CEREBRO_COMPLETE_DOCUMENTATION.md](CEREBRO_COMPLETE_DOCUMENTATION.md)
- **Issues**: [GitHub Issues](https://github.com/yourusername/cerebro/issues)
- **Email**: your.email@example.com

---

**Built with AI for MMM practitioners** ğŸ§ 

